{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n抓取百度贴吧--逆水寒吧的基本内容\\n爬虫线路：requests - bs4\\nPython版本：3.6\\nOS：windows 10 \\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "抓取百度贴吧--逆水寒吧的基本内容\n",
    "爬虫线路：requests - bs4\n",
    "Python版本：3.6\n",
    "OS：windows 10 \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取网页\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r  = requests.get(url,timeout=38)\n",
    "        r.raise_for_status()\n",
    "        r.endcodding = r.apparent_endcodding\n",
    "        return r.text\n",
    "    except:\n",
    "        return ' ERROR '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取信息\n",
    "def get_content(url):\n",
    "    \n",
    "    #初始化一个列表用来保存所有帖子信息\n",
    "    posts = []\n",
    "    #下载网页到本地\n",
    "    html = get_html(url)\n",
    "    \n",
    "    #做一锅汤\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    \n",
    "    #找到所有具有 'j_thread_list clearfix'属性的li标签，存入列表\n",
    "    liTags = soup.find_all('li', attrs ={'class':' j_thread_list clearfix'} )\n",
    "    \n",
    "    #寻找信息\n",
    "    for li in liTags:\n",
    "        #字典存储文章信息\n",
    "        post = {}\n",
    "        \n",
    "        #使用try 防止爬虫找不到信息从而停止运行\n",
    "        try :\n",
    "            post['title'] = li.find('a', attrs = {'class':'threadlist_title pull_left j_th_tit '}).text.strip()\n",
    "            post['link'] = \"http://tieba.baidu.com/\"+li.find('a', attrs={'class': 'threadlist_title pull_left j_th_tit '})['href']\n",
    "            post['name'] = li.find('span', attrs={'class': 'tb_icon_author '}).text.strip()\n",
    "            post['time'] = li.find('span', attrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            post['replyNum'] = li.find('span', sttrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            posts.append(post)\n",
    "        except:\n",
    "            print('出问题了~')\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的nshba.txt文件中\n",
    "    '''\n",
    "    with open('nshba.txt','a+') as f:\n",
    "        for post in dict:\n",
    "            f.write('标题:{}\\t 链接:{}\\t 发帖人:{}\\t 最后回复时间:{}\\t 回复数量:{}\\n'.foramt(\n",
    "                post['title'], post['link'], post['name'], post['time'], post['replyNum']))\n",
    "            \n",
    "        print('当前网页爬取完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(url_0, deep):\n",
    "    \n",
    "    url_list = [] #将要爬取的url存入列表\n",
    "\n",
    "    for i in range(0,deep):\n",
    "        url_list.append(url_0 +'&pn=' + str(50 * i))\n",
    "    print('所有网页已经下载到本地了，开始爬虫吧')\n",
    "    \n",
    "    #循环写入信息\n",
    "    for url in url_list:\n",
    "        post = get_content(url)\n",
    "        outToFile(post)\n",
    "    print('所有信息都爬取并保存完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有网页已经下载到本地了，开始爬虫吧\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有信息都爬取并保存完毕\n"
     ]
    }
   ],
   "source": [
    "url_0 = 'https://tieba.baidu.com/f?kw=%E9%80%86%E6%B0%B4%E5%AF%92ol&ie=utf-8'\n",
    "deep = 10\n",
    "start(url_0, deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
