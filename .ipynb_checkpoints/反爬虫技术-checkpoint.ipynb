{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反爬虫技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、通过User-Agent来控制访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论是浏览器还是爬虫程序，在向服务器发起网络请求的时候，都会发过去一个头文件：headers,例如知乎的requests headers:    \n",
    "<pre/>\n",
    "Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8\n",
    "Accept-Encoding:gzip, deflate, sdch, br\n",
    "Accept-Language:zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4,da;q=0.2,la;q=0.2\n",
    "Cache-Control:max-age=0\n",
    "Connection:keep-alive\n",
    "Cookie: **\n",
    "Host:https://www.zhihu.com/\n",
    "Referer:https://www.zhihu.com/\n",
    "Upgrade-Insecure-Requests:1\n",
    "User-Agent:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\n",
    "</pre>\n",
    "\n",
    "这里的大多数字段都是向服务器**表明身份**用的    \n",
    "对于爬虫程序来说，最需要注意的字段就是： \"**User-Agent:**\"      \n",
    "很多网站会建立User-Agent白名单，只有属于正常范围的User-Agent才能够正常访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\r\n",
      "<head><title>400 Bad Request</title></head>\r\n",
      "<body bgcolor=\"white\">\r\n",
      "<center><h1>400 Bad Request</h1></center>\r\n",
      "<hr><center>openresty</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###例如直接访问知乎\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def get_page(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 30)\n",
    "        r.raise_for_status\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return('Wrong in get_page()')\n",
    "    \n",
    "print(get_page('https://www.zhihu.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们的请求被拒绝了，并且返回了一个400的错误代码    \n",
    "> 200 成功，代表服务器正常相应    \n",
    "> 400 错误请求，代表服务器无法解析该请求    \n",
    "> 403 未找到，服务器找不到请求的网页    \n",
    "> 404 未找到，代表页面未找到    \n",
    "> 500 服务器内部错误，代表服务器内部发生错误    \n",
    "> 更多状态码参考 [HTTP基本原理](https://cuiqingcai.com/5465.html)\n",
    "\n",
    "\n",
    "这里是因为我们本身的requests库的headers是这样的： \n",
    "<pre/>\n",
    "{'Date': 'Sun, 05 May 2019 10:44:19 GMT', \n",
    "'Content-Type': 'text/html', \n",
    "'Content-Length': '170', \n",
    "'Connection': 'keep-alive', \n",
    "'Set-Cookie': 'tgw_l7_route=80f350dcd7c650b07bd7b485fcab5bf7; Expires=Sun, 05-May-2019 10:59:14 GMT; Path=/, _xsrf=qsWD0qeepnCXUb7N7hNKnweLHiTYw9sD; path=/; domain=zhihu.com; expires=Thu, 21-Oct-21 10:44:19 GMT', 'Server': 'ZWS', 'Vary': 'Accept-Encoding'}\n",
    "</pre>\n",
    "\n",
    "这里没有User-Agent字段，自然不被知乎的服务器所接受"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Sun, 05 May 2019 10:44:19 GMT', 'Content-Type': 'text/html', 'Content-Length': '170', 'Connection': 'keep-alive', 'Set-Cookie': 'tgw_l7_route=80f350dcd7c650b07bd7b485fcab5bf7; Expires=Sun, 05-May-2019 10:59:14 GMT; Path=/, _xsrf=qsWD0qeepnCXUb7N7hNKnweLHiTYw9sD; path=/; domain=zhihu.com; expires=Thu, 21-Oct-21 10:44:19 GMT', 'Server': 'ZWS', 'Vary': 'Accept-Encoding'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看requests的headers\n",
    "def get_headers(url):\n",
    "    r = requests.get(url, timeout = 30)\n",
    "    return r.headers\n",
    "\n",
    "get_headers('https://www.zhihu.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决办法**：   \n",
    "我们可以自己设置一下User-Agent，或者更好的是，我们从一系列的User-Agent里随机挑选出一个符合标准的使用，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent():\n",
    "    '''\n",
    "    模拟headers的User-Agent字段\n",
    "    返回一个随机的User-Agent字典类型的键值对\n",
    "    '''\n",
    "    \n",
    "    agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',\n",
    "              'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;',\n",
    "              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv,2.0.1) Gecko/20100101 Firefox/4.0.1',\n",
    "              'Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11',\n",
    "              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',\n",
    "              'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)']\n",
    "    fakeheader = {}\n",
    "    fakeheader['User-Agent'] = agents[random.randint(0,len(agents)-1)]\n",
    "    return fakeheader\n",
    "\n",
    "def get_page(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 30, headers = get_agent())\n",
    "        r.raise_for_status\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return('Something Wrong in get_page()')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1\" />\n",
      "    <meta name=\"google-site-verification\" content=\"FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg\" />\n",
      "    <link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://static.zhihu.com/static/favicon.ico\" />\n",
      "    <style>\n",
      "      body {\n",
      "        margin: 0;\n",
      "        font-family: 'Helvetica Neue', Helvetica, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif;\n",
      "        font-size: 15px;\n",
      "        color: #404040;\n",
      "      }\n",
      "\n",
      "      .Header {\n",
      "        overflow: hidden;\n",
      "        background-color: #fff;\n",
      "        box-shadow: 0 3px 4px rgba(0, 0, 0, 0.06);\n",
      "      }\n",
      "\n",
      "      .Header-inner {\n",
      "        width: 1008px;\n",
      "        margin: 0 auto;\n",
      "        padding: 0 16px;\n",
      "      }\n",
      "\n",
      "      .Header-logo {\n",
      "        width: 64px;\n",
      "        height: 30px;\n",
      "        margin: 15px 0;\n",
      "      }\n",
      "\n",
      "      .Header-logo img {\n",
      "        display: block;\n",
      "        width: 100%;\n",
      "        height: 100%;\n",
      "      }\n",
      "\n",
      "      .Compatibility {\n",
      "        width: 1008px;\n",
      "        margin: 0 auto;\n",
      "        padding: 16px;\n",
      "      }\n",
      "\n",
      "      .Compatibility-icon {\n",
      "        width: 112px;\n",
      "        height: 83px;\n",
      "        margin: 64px auto;\n",
      "      }\n",
      "\n",
      "      .Compatibility-icon img {\n",
      "        display: block;\n",
      "        width: 100%;\n",
      "        height: 100%;\n",
      "      }\n",
      "\n",
      "      .Compatibility-tip {\n",
      "        font-size: 24px;\n",
      "        @mixin bold;\n",
      "        text-align: center;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers {\n",
      "        overflow: hidden;\n",
      "        margin-top: 64px;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-item {\n",
      "        position: relative;\n",
      "        float: left;\n",
      "        width: 50%;\n",
      "        padding: 16px 0;\n",
      "      }\n",
      "\n",
      "      a.CompatibilityBrowsers-item {\n",
      "        text-decoration: none;\n",
      "        color: #3e7ac2;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-divider {\n",
      "        position: absolute;\n",
      "        right: 0;\n",
      "        top: 24px;\n",
      "        height: 64px;\n",
      "        border-right: 1px solid #ebeef5;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-icon {\n",
      "        width: 80px;\n",
      "        height: 80px;\n",
      "        margin: 0 auto;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-icon img {\n",
      "        display: block;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-ieIcon img {\n",
      "        width: 88px;\n",
      "        height: 81px;\n",
      "        margin-left: -8px;\n",
      "        margin-top: -1px;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-chromeIcon img {\n",
      "        width: 83px;\n",
      "        height: 83px;\n",
      "        margin: -1.5px;\n",
      "      }\n",
      "\n",
      "      .CompatibilityBrowsers-text {\n",
      "        margin-top: 32px;\n",
      "        text-align: center;\n",
      "      }\n",
      "    </style>\n",
      "  </head>\n",
      "  <body>\n",
      "    <div class=\"Header\">\n",
      "      <div class=\"Header-inner\">\n",
      "        <div class=\"Header-logo\">\n",
      "          <img src=\"/compatibility/images/logo@2x.png\" srcset=\"/compatibility/images/logo.png 1x, /compatibility/images/logo@2x.png 2x, /compatibility/images/logo@3x.png 3x\" />\n",
      "        </div>\n",
      "      </div>\n",
      "    </div>\n",
      "    <div class=\"Compatibility\">\n",
      "      <div class=\"Compatibility-icon\">\n",
      "        <img src=\"/compatibility/images/icon@2x.png\" srcset=\"/compatibility/images/icon.png 1x, /compatibility/images/icon@2x.png 2x, /compatibility/images/icon@3x.png 3x\" />\n",
      "      </div>\n",
      "      <div class=\"Compatibility-tip\">你正在使用的浏览器版本过低，将不能正常浏览和使用知乎。</div>\n",
      "      <div class=\"CompatibilityBrowsers\">\n",
      "        <a href=\"https://www.microsoft.com/zh-cn/download/internet-explorer.aspx\" class=\"CompatibilityBrowsers-item\">\n",
      "          <div class=\"CompatibilityBrowsers-icon CompatibilityBrowsers-ieIcon\">\n",
      "            <img src=\"/compatibility/images/internet-explorer@2x.png\" srcset=\"/compatibility/images/internet-explorer.png 1x, /compatibility/images/internet-explorer@2x.png 2x, /compatibility/images/internet-explorer@3x.png 3x\" />\n",
      "          </div>\n",
      "          <div class=\"CompatibilityBrowsers-text\">升级 IE 浏览器</div>\n",
      "          <div class=\"CompatibilityBrowsers-divider\"></div>\n",
      "        </a>\n",
      "        <a href=\"http://www.google.com/chrome/\" class=\"CompatibilityBrowsers-item\">\n",
      "          <div class=\"CompatibilityBrowsers-icon CompatibilityBrowsers-chromeIcon\">\n",
      "            <img src=\"/compatibility/images/google-chrome@2x.png\" srcset=\"/compatibility/images/google-chrome.png 1x, /compatibility/images/google-chrome@2x.png 2x, /compatibility/images/google-chrome@3x.png 3x\" />\n",
      "          </div>\n",
      "          <div class=\"CompatibilityBrowsers-text\">使用 Google Chrome 浏览器</div>\n",
      "        </a>\n",
      "    </div>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_page('https://www.zhihu.com/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、通过IP限制来反爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一个固定的ip在短时间内，快速大量的访问网站，那自然就会引起注意，管理员可以通过一些手段把这个ip给封了，爬虫自然就做不了什么了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决办法：**   \n",
    "比较成熟的方式是：IP代理池    \n",
    "简单的说就是通过ip代理，从不同的ip进行访问，这样就不会被封掉ip了    \n",
    "可是ip代理的获取，本身就是一个很麻烦的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy():\n",
    "    '''\n",
    "    简答模拟代理池\n",
    "    返回一个字典类型的键值对，\n",
    "    '''\n",
    "    proxy = [\"http://116.211.143.11:80\",\n",
    "             \"http://183.1.86.235:8118\",\n",
    "             \"http://183.32.88.244:808\",\n",
    "             \"http://121.40.42.35:9999\",\n",
    "             \"http://222.94.148.210:808\"]\n",
    "    fakepxs = {}\n",
    "    fakepxs['http'] = proxy[random.randint(0, len(proxy)-1)]\n",
    "    \n",
    "    return fakepxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、通过JS脚本来防止爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个可以说是终极的办法了，因为，爬虫终归只是一段程序，他并不能像人一样去应对各种变化，如验证码，滑动解锁之类的。    \n",
    "举个例子：我曾经想爬一个分享百度云电影的网站，但是在进入网站之前，他会有一个验证页面来验证你是不是机器。    \n",
    "他是怎么验证的呢：   \n",
    "> 他会通过js代码生成一大段随机的数字，然后要求浏览器通过js的运算得出这一串数字的和，再返回给服务器.      \n",
    "\n",
    "可想而知的是，这么简单和最基础的一个验证步骤，我们从前写的代码是完成不了的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解决办法：PhantomJS**\n",
    "\n",
    "PhantomJS是一个Python包，他可以在没有图形界面的情况下，完全模拟一个”浏览器“，对你没看错，就是浏览器，这样的话，js脚本验证什么的再也不是问题了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、通过robots.txt来限制爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说起来，世界上做爬虫最大最好的就是Google了，搜索引擎本身就是一个超级大的爬虫，Google开发出来爬虫24h不间断的在网上爬取着新的信息，并返回给数据库，但是这些搜索引擎的爬虫都遵守着一个协议：robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为小写。robots.txt应放置于网站的根目录下。如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据（Metadata，又称元数据）。\n",
    "robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。注意robots.txt是用字符串比较来确定是否获取URL，所以目录末尾有与没有斜杠“/”表示的是不同的URL。robots.txt允许使用类似\"Disallow: *.gif\"这样的通配符[1][2]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是京东的'robots.txt'\n",
    "# https://www.jd.com/robots.txt\n",
    "User-agent: * \n",
    "Disallow: /?* \n",
    "Disallow: /pop/*.html \n",
    "Disallow: /pinpai/*.html?* \n",
    "User-agent: EtaoSpider \n",
    "Disallow: / \n",
    "User-agent: HuihuiSpider \n",
    "Disallow: / \n",
    "User-agent: GwdangSpider \n",
    "Disallow: / \n",
    "User-agent: WochachaSpider \n",
    "Disallow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* User-agent: \\* ：指的是对于任何的网络爬虫来源（定义为user-agent），也就是说无论你是什么样的网络爬虫都应该遵守如下的协议。    \n",
    "\n",
    "* Disallow: /?\\* ：不允许所有爬虫访问以问号开头的路径。   \n",
    "\n",
    "* Disallow: /pop/\\*.html ：不允许所有爬虫访问pop目录下的所有HTML页面。\n",
    "\n",
    "* Disallow: /pinpai/\\*.html?\\* ：符合这个通配符的内容也是不允许任何网络爬虫访问的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   可以看到，jd的robots协议里明确的指出四个\"user-agent”是禁止访问的，\n",
    "事实上，这四个user-agent也是四个臭名昭著的恶性爬虫!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
