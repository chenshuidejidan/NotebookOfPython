{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、BeautifulSoup 爬虫\n",
    "\n",
    "   * requests 用来获取页面内容\n",
    "   * BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 安装           \n",
    "pip install requests         \n",
    "pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.爬取链家租房的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cd.lianjia.com/zufang/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_div = soup.find_all('div',  class_ = \"content__list--item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [div.a.get('herf') for div in links_div]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码优化\n",
    "\n",
    "#获取url页面内容\n",
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    retuen soup\n",
    "    \n",
    "def get_links(link_url):\n",
    "    soup = get_page(link_url)\n",
    "    links_div = soup.find_all('div',  class_ = \"content__list--item\")\n",
    "    links = [div.a.get('herf') for div in links_div]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取房屋的数据\n",
    "def get_house_info(house_url):\n",
    "    #house_url = ''\n",
    "    soup = get_page(house_url)\n",
    "    price = soup.find('span', class_ = 'total').text\n",
    "    unit = soup.find('span', class_ = 'unit').text.strip()  #价格单位并去除首尾格式\n",
    "    house_info = soup.find_all('p', class_ = 'lf')\n",
    "    area= house_info[0].text[3:]\n",
    "    layout = house_info[1].text[5:]\n",
    "    floor = house_info[2].text[3:]\n",
    "    direction = house_info[3].text[5:]\n",
    "    info = {\n",
    "        '价格': price,\n",
    "        '单位':unit,\n",
    "        '面积':area,\n",
    "        '户型':layout,\n",
    "        '楼层':floor,\n",
    "        '朝向':direction\n",
    "    }\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.爬取百度贴吧逆水寒吧的帖子信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取网页\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        # 这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用如下代码，但是该编码方式不一定准确：\n",
    "        #r.encoding = r.apparent_encoding\n",
    "        r.encoding = 'utf-8'\n",
    "        return r.text\n",
    "        #html=r.content\n",
    "        #html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "        #return html_doc\n",
    "    except:\n",
    "        return \" ERROR \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们知道 r.text 返回的是Unicode型的数据。    \n",
    "使用r.content返回的是bytes型的数据。   \n",
    "也就是说，如果你想取文本，可以通过r.text。  \n",
    "如果想取图片，文件，则可以通过r.content。   \n",
    "\n",
    "<pre/>\n",
    "    r = requests.get(url, timeout = 30)\n",
    "    r.raise_for_status()\n",
    "    html=r.content\n",
    "    html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    return html_doc\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取信息\n",
    "def get_content(url):\n",
    "    \n",
    "    #初始化一个列表用来保存所有帖子信息\n",
    "    posts = []\n",
    "    #下载网页到本地\n",
    "    html_doc = get_html(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    #a = requests.get(url)\n",
    "    #html=a.content\n",
    "    #html_doc=str(html,'utf-8') \n",
    "    #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    #做一锅汤\n",
    "    soup = BeautifulSoup(html_doc)\n",
    "\n",
    "    #找到所有具有 'j_thread_list clearfix'属性的li标签，存入列表\n",
    "    liTags = soup.find_all('li', attrs={'class':'j_thread_list'} )\n",
    "\n",
    "    #寻找信息\n",
    "\n",
    "    for li in liTags:\n",
    "        #字典存储文章信息\n",
    "        post = {}\n",
    "\n",
    "        #使用try 防止爬虫找不到信息从而停止运行\n",
    "        try :\n",
    "            '''\n",
    "            post['title'] = li.find('a', attrs = {'class':'threadlist_title pull_left j_th_tit '}).text.strip()\n",
    "            post['link'] = \"http://tieba.baidu.com/\"+li.find('a', attrs={'class': 'threadlist_title pull_left j_th_tit '})['href']\n",
    "            post['name'] = li.find('span', attrs={'class': 'tb_icon_author '}).text.strip()\n",
    "            post['time'] = li.find('span', attrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            post['replyNum'] = li.find('span', sttrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            '''\n",
    "            post['title'] = li.find('a', class_='j_th_tit').string\n",
    "            post['link'] = 'http://tieba.baidu.com' + li.find('a', class_='j_th_tit').get('href')\n",
    "            post['name'] = li.find('a', class_= 'j_user_card').string\n",
    "            post['time'] = li.find('span', class_='is_show_create_time').string\n",
    "            post['replyNum'] = li.find('span', class_='threadlist_rep_num').string\n",
    "            posts.append(post)\n",
    "        except:\n",
    "            print('出问题了~')\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的nshba.txt文件中\n",
    "    '''\n",
    "    with open('nshba.txt','a+',encoding='utf-8') as f:\n",
    "        for post in dict:\n",
    "            f.write('标题:{}\\t 链接:{}\\t 发帖人:{}\\t 最后回复时间:{}\\t 回复数量:{}\\n'.format(\n",
    "                post['title'], post['link'], post['name'], post['time'], post['replyNum']))\n",
    "            \n",
    "        print('当前网页爬取完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(url_0, deep):\n",
    "    \n",
    "    url_list = [] #将要爬取的url存入列表\n",
    "\n",
    "    for i in range(0,deep):\n",
    "        url_list.append(url_0 +'&pn=' + str(50 * i))\n",
    "    print('所有网页已经下载到本地了，开始爬虫吧')\n",
    "    \n",
    "    #循环写入信息\n",
    "    for url in url_list:\n",
    "        post = get_content(url)\n",
    "        outToFile(post)\n",
    "    print('所有信息都爬取并保存完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有网页已经下载到本地了，开始爬虫吧\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有信息都爬取并保存完毕\n"
     ]
    }
   ],
   "source": [
    "url_0 = 'https://tieba.baidu.com/f?kw=%E9%80%86%E6%B0%B4%E5%AF%92ol&ie=utf-8'\n",
    "deep = 3\n",
    "start(url_0, deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
