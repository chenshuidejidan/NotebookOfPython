{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 爬虫知识\n",
    "\n",
    "   * requests 用来获取页面内容\n",
    "   * BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 安装           \n",
    "pip install requests         \n",
    "pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Beautiful Soup的四大对象种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:    \n",
    "* Tag    \n",
    "* NavigableString   \n",
    "* BeautifulSoup   \n",
    "* Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\" name=\"dromouse\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    <!-- Elsie -->\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# soup.prettify() 函数：格式化输出soup对象内容\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Tag   \n",
    "\n",
    "Tag就是HTML中的一个个标签。例如上面定义的html里的head，title，a等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>The Dormouse's story</title></head>\n"
     ]
    }
   ],
   "source": [
    "print(soup.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Tag，她有两个重要的属性，分别是**name和attrs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[document]\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "# name\n",
    "\n",
    "print(soup.name)           #soup对象本身也有name，为[document]\n",
    "print(soup.head.name)  #其他内部标签，name即为标签本身的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['title'], 'name': 'dromouse'}\n"
     ]
    }
   ],
   "source": [
    "# attrs\n",
    "\n",
    "print(soup.p.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里原html中有 \n",
    "\n",
    "    <p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
    "           \n",
    "\n",
    "我们把p标签的所有属性打印输出了出来，得到一个字典  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dromouse\n"
     ]
    }
   ],
   "source": [
    "#如果我们想要获取单独某个属性，就可以采用字典的方式去访问\n",
    "print(soup.p['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dromouse\n"
     ]
    }
   ],
   "source": [
    "#也可以使用get方法\n",
    "print(soup.p.get('name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) NavigableString    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#既然已经得到了标签，那么获取标签里的文字也很简单，用 .string 即可\n",
    "soup.p.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NavigableString其实就是可以遍历的字符串\n",
    "type(soup.p.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup 对象表示的是一个文档的全部内容。    \n",
    "大部分时候,可以把它当作 Tag 对象，是一个特殊的 Tag，我们可以分别获取它的类型，名称，以及属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[document]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment 对象是一个特殊类型的 NavigableString 对象，其实输出的内容仍然不包括注释符号，但是如果不好好处理它，可能会对我们的文本处理造成意想不到的麻烦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>\n",
      " Elsie \n",
      "<class 'bs4.element.Comment'>\n"
     ]
    }
   ],
   "source": [
    "print(soup.a)\n",
    "print(soup.a.string)\n",
    "print(type(soup.a.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a标签里的内容实际上是注释，但是我们利用 .string 来输出它的内容，发现注释符号去掉了，所以会带来不必要的麻烦\n",
    "\n",
    "输出的类型是一个 Comment 类型，所以，我们在使用前最好做一下判断："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment(  Elsie  )\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "#注意：此处若不引用bs4库，会报错        name 'bs4' is not defined\n",
    "#因为 from bs4 import BeautifulSoup          import的只是bs4库的一小部分，\n",
    "#而if语句用到的是没有导入的一部分，就再加上bs4库的引用，再运行就没有再报错了。\n",
    "\n",
    "if (type(soup.a.string)!=bs4.element.Comment):\n",
    "    print(soup.a.string)\n",
    "else:\n",
    "    print('Comment(',soup.a.string,')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、遍历文档树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)直接子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.contents**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag的 .contents 属性可以将tag的子节点以列表的方式输出\n",
    "soup.head.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出方式为列表，可以使用列表索引来获取它的某个元素\n",
    "soup.head.contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.children**    \n",
    "它返回的不是一个list，而是迭代器，我们可以通过遍历获取所有子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x10c0a92ee80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .children是一个 list 生成器对象\n",
    "soup.head.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 遍历一下\n",
    "for child in soup.body.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 所有子孙节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.descendants**    \n",
    "\n",
    ".contents 和 .children 属性仅包含tag的直接子节点， .descendants属性可以对所有tag的子孙节点进行循环递归，和 children 类似，我们也需要遍历获取其内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in soup.descendants:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行发现所有节点都被打印出来了，先是最外层的HTML标签，其次从head标签一个个剥离，以此类推"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 节点内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.string**   \n",
    "\n",
    "如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点。如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同。\n",
    "\n",
    "即：如果一个标签里面没有标签了，那么 .string 就会返回标签里面的内容。如果标签里面只有唯一的一个标签了，那么 .string 也会返回最里面的内容。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n",
      "The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "print(soup.head.string)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果tag包含了多个子节点，tag就无法确定string方法应该调用哪个子节点的内容，**.string的输出结果是None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.html.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 多个内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.strings**    \n",
    "获取多个内容，不过需要遍历获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'\\n'\n",
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'Once upon a time there were three little sisters; and their names were\\n'\n",
      "',\\n'\n",
      "'Lacie'\n",
      "' and\\n'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'\\n'\n",
      "'...'\n",
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.strings:\n",
    "    print(repr(string))    #repr函数，返回对象的string格式(包括换行符等)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.stripped_strings**    \n",
    "可以去除多余空白内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Dormouse's story\"\n",
      "\"The Dormouse's story\"\n",
      "'Once upon a time there were three little sisters; and their names were'\n",
      "','\n",
      "'Lacie'\n",
      "'and'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 父节点\n",
    "\n",
    "* **.parent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'body'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = soup.p\n",
    "p.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.head.title.string\n",
    "content.parent.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 全部父节点\n",
    "* **.parents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "head\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "content = soup.head.title.string\n",
    "for parent in content.parents:\n",
    "    print(parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 兄弟节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.next_sibling 和 .previous_sibling**    \n",
    "\n",
    "注意：实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "print(repr(soup.p.next_sibling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.p.prev_sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 全部兄弟节点   \n",
    "* **.next_siblings 和 .previous_siblings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 前后节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.next_element 和 .previous_element**    \n",
    "与 .next_sibling 和 .previous_sibling 相区别，它并不是针对兄弟节点，而是在所有节点，不分层次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) 前后所有节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.next_elements 和 .previous_elements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、搜索文档树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) find_all(name, attrs, recursive, text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_all 方法搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.name参数：查找所有名字为name的tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **传字符串**：最简单的过滤器是字符串，在搜索方法中传入一个字符串参数，Beautiful Soup会查找与字符串完整匹配的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **传正则表达式**：如果传入正则表达式，Beautiful Soup会通过正则表达式的 **match()** 来匹配内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for tag in soup.find_all(re.compile(\"^b\")):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **传列表**：如果传入列表参数,Beautiful Soup会将与**列表中任一元素匹配**的内容返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>,\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all([\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **传True**：True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "head\n",
      "title\n",
      "body\n",
      "p\n",
      "b\n",
      "p\n",
      "a\n",
      "a\n",
      "a\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **传方法**：如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_class_but_no_id(tag):\n",
    "    return tag.has_attr('class') and (not tag.has_attr('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>,\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>,\n",
       " <p class=\"story\">...</p>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(has_class_but_no_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.keyword参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id='link2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(href=re.compile(\"elsie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(href=re.compile(\"elsie\"), id='link1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用class过滤，可以加下划线！\n",
    "soup.find_all(\"a\", class_=\"sister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-71-a66666d9b6f1>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-a66666d9b6f1>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    data_soup.find_all(data-foo=\"value\")\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "# 有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性\n",
    "data_soup = BeautifulSoup('<div data-foo=\"value\">foo!</div>')\n",
    "data_soup.find_all(data-foo=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div data-foo=\"value\">foo!</div>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#但是可以通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag\n",
    "data_soup = BeautifulSoup('<div data-foo=\"value\">foo!</div>')\n",
    "data_soup.find_all(attrs={\"data-foo\": \"value\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.text参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 text 参数可以搜搜文档中的字符串内容。与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Elsie ']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(text=\" Elsie \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lacie', 'Tillie']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(text=[\"Tillie\", \"Elsie\", \"Lacie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Dormouse's story\", \"The Dormouse's story\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(text=re.compile(\"Dormouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.limit参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.recursive 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) find(name , attrs , recursive , text , **kwargs)    \n",
    "\n",
    "它与 find_all() 方法唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) find_parents()  find_parent()    \n",
    "\n",
    "find_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等. find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) find_next_siblings()  find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) find_previous_siblings()  find_previous_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) find_all_next()  find_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) find_all_previous() 和 find_previous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、CSS选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CSS中，我们使用CSS选择器来定位节点。   \n",
    "\n",
    "例如，div 节点的 id 为 container，那么就可以表示为 **#container**，其中 # 开头代表选择  id，其后紧跟id的名称。   \n",
    "另外，如果我们想选择 class 为 wrapper 的节点，便可以使用 **.wrapper**，这里以点（.）开头代表选择 class ，其后紧跟 class 的名称。   \n",
    "另外，还有一种选择方式，那就是根据标签名筛选，例如想选择二级标题，直接用h2即可。这是最常用的3种表示，分别是根据id、class、标签名筛选。\n",
    "\n",
    "**CSS选择器还支持嵌套选择**，各个选择器之间加上**空格**分隔开便可以代表**嵌套**关系。    \n",
    "如 **#container .wrapper p** 则代表先选择 id 为 container 的节点，然后选中其内部的 class 为 wrapper 的节点，然后再进一步选中其内部的 p 节点。\n",
    "\n",
    "**如果不加空格，则代表并列关系**，如 **div#container .wrapper p.text** 代表先选择 id 为 container 的div节点，然后选中其内部的class为wrapper的节点，再进一步选中其内部的class为text的p节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 通过标签名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('title') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 通过类名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('.sister')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 通过 id 名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('#link1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 组合查找     \n",
    "组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('p #link1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接子标签查找\n",
    "soup.select(\"head > title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 属性查找    \n",
    "查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a[class=\"sister\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a[href=\"http://example.com/elsie\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，属性仍然可以与上述查找方式组合，不在同一节点的空格隔开，同一节点的不加空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"><!-- Elsie --></a>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('p a[href=\"http://example.com/elsie\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "The Dormouse's story\n",
      "The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "print(type(soup.select('title')))\n",
    "print(soup.select('title')[0].get_text())\n",
    " \n",
    "for title in soup.select('title'):\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.爬取链家租房的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取url页面内容\n",
    "def get_page(url,headers):\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    return soup\n",
    "\n",
    "#获取整页的每个租房信息的链接\n",
    "def get_links(link_url,headers):\n",
    "    soup = get_page(link_url,headers)\n",
    "    links_a = soup.find_all('a' , attrs = {'class':'content__list--item--aside'})\n",
    "    links = [('https://cd.lianjia.com'+ link.get('href')) for link in links_a]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取房屋的数据\n",
    "def get_house_info(house_url):\n",
    "    headers['Referer']=house_url\n",
    "    soup = get_page(house_url,headers)\n",
    "    try:\n",
    "        name = soup.find('p', class_ = 'content__title').text\n",
    "        price = soup.find('p', class_ = 'content__aside--title').text.strip()\n",
    "        #text与.string的区别 https://zhuanlan.zhihu.com/p/30911642\n",
    "        #个人建议基本用.text不容易出错\n",
    "\n",
    "        #unit = soup.find('span', class_ = 'unit').text.strip()  #价格单位并去除首尾格式\n",
    "        house_tag0 = soup.find('p', class_ = 'content__aside--tags').text.strip().split('\\n')\n",
    "        house_tag = ''\n",
    "        for tag in house_tag0 :\n",
    "            house_tag = house_tag + tag + ' '\n",
    "\n",
    "        house_info = soup.find('p', class_ = 'content__article__table').text.strip().split('\\n')\n",
    "        way = house_info[0]\n",
    "        layout = house_info[1]\n",
    "        area = house_info[2]\n",
    "        direction = house_info[3]\n",
    "\n",
    "        #楼层信息在房屋信息栏里，需要切片访问\n",
    "        floor = soup.find('div',  class_  =  'content__article__info').text.strip().split('\\n')[9][3:]\n",
    "    except:\n",
    "        return {}\n",
    "    info = {\n",
    "        '标题':name,\n",
    "        '价格': price,\n",
    "        '标签':house_tag,        \n",
    "        '租法':way,\n",
    "        '户型':layout,\n",
    "        '面积':area,        \n",
    "        '朝向':direction, \n",
    "        '楼层':floor,\n",
    "    }\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的cdlianjia.txt文件中\n",
    "    '''\n",
    "    if dict:\n",
    "        with open('cdlianjia.txt','a+',encoding='utf-8') as f:\n",
    "            f.write('标题:{}\\t 价格:{}\\t 标签:{}\\t 租法:{}\\t 户型:{}\\t 面积:{}\\t 朝向:{}\\t 楼层:{}\\n'.\n",
    "                    format(dict['标题'],dict['价格'], dict['标签'], dict['租法'], dict['户型'], \n",
    "                           dict['面积'], dict['朝向'], dict['楼层']))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(deep):\n",
    "    #构造headers模拟真实浏览器访问\n",
    "    headers = {\n",
    "        'Host': \"cd.lianjia.com\",\n",
    "        'Accept': \"application/json, text/javascript, */*; q=0.01\",\n",
    "        'Accept-Encoding': \"gzip, deflate, br\",\n",
    "        'Accept-Language': \"zh-CN,zh;q=0.9\",\n",
    "        'Referer': 'https://cd.lianjia.com/zufang/',\n",
    "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\",\n",
    "        'Connection': \"keep-alive\",\n",
    "    }\n",
    "    url_list = [('https://cd.lianjia.com/zufang/'+'pg'+str(k) +'/') for k in range(1,deep+1)]\n",
    "    print('已获取链接，开始爬虫')\n",
    "    for url in url_list:\n",
    "        headers['Referer'] = url\n",
    "        links = get_links(url, headers)  #得到每个房屋的链接        \n",
    "        for link in links :\n",
    "            house_info = get_house_info(link)  #得到单个租房信息字典\n",
    "            outToFile(house_info)\n",
    "        print('当前网页爬取完毕')\n",
    "    print('所有页已爬取完毕')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已获取链接，开始爬虫\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有页已爬取完毕\n"
     ]
    }
   ],
   "source": [
    "start(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **错误总结：**    \n",
    "1. get('href')写成了get('herf')，于是截取不到链接    \n",
    "2. 'NoneType' object has no attribute 'text'错误，是我们提取的时候有空的信息，要进行判断处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.爬取百度贴吧逆水寒吧的帖子信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取网页\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        # 这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用如下代码，但是该编码方式不一定准确：\n",
    "        #r.encoding = r.apparent_encoding\n",
    "        r.encoding = 'utf-8'\n",
    "        return r.text\n",
    "        #html=r.content\n",
    "        #html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "        #return html_doc\n",
    "    except:\n",
    "        return \" ERROR \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们知道 r.text 返回的是Unicode型的数据。    \n",
    "使用r.content返回的是bytes型的数据。   \n",
    "也就是说，如果你想取文本，可以通过r.text。  \n",
    "如果想取图片，文件，则可以通过r.content。   \n",
    "\n",
    "<pre/>\n",
    "    r = requests.get(url, timeout = 30)\n",
    "    r.raise_for_status()\n",
    "    html=r.content\n",
    "    html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    return html_doc\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取信息\n",
    "def get_content(url):\n",
    "    \n",
    "    #初始化一个列表用来保存所有帖子信息\n",
    "    posts = []\n",
    "    #下载网页到本地\n",
    "    html_doc = get_html(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    #a = requests.get(url)\n",
    "    #html=a.content\n",
    "    #html_doc=str(html,'utf-8') \n",
    "    #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    #做一锅汤\n",
    "    soup = BeautifulSoup(html_doc)\n",
    "\n",
    "    #找到所有具有 'j_thread_list clearfix'属性的li标签，存入列表\n",
    "    liTags = soup.find_all('li', attrs={'class':'j_thread_list'} )\n",
    "\n",
    "    #寻找信息\n",
    "\n",
    "    for li in liTags:\n",
    "        #字典存储文章信息\n",
    "        post = {}\n",
    "\n",
    "        #使用try 防止爬虫找不到信息从而停止运行\n",
    "        try :\n",
    "            '''\n",
    "            post['title'] = li.find('a', attrs = {'class':'threadlist_title pull_left j_th_tit '}).text.strip()\n",
    "            post['link'] = \"http://tieba.baidu.com/\"+li.find('a', attrs={'class': 'threadlist_title pull_left j_th_tit '})['href']\n",
    "            post['name'] = li.find('span', attrs={'class': 'tb_icon_author '}).text.strip()\n",
    "            post['time'] = li.find('span', attrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            post['replyNum'] = li.find('span', sttrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            '''\n",
    "            post['title'] = li.find('a', class_='j_th_tit').text\n",
    "            post['link'] = 'http://tieba.baidu.com' + li.find('a', class_='j_th_tit').get('href')\n",
    "            post['name'] = li.find('a', class_= 'j_user_card').text\n",
    "            post['time'] = li.find('span', class_='is_show_create_time').text\n",
    "            post['replyNum'] = li.find('span', class_='threadlist_rep_num').text\n",
    "            posts.append(post)\n",
    "        except:\n",
    "            print('出问题了~')\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的nshba.txt文件中\n",
    "    '''\n",
    "    with open('nshba.txt','a+',encoding='utf-8') as f:\n",
    "        for post in dict:\n",
    "            f.write('标题:{}\\t 链接:{}\\t 发帖人:{}\\t 最后回复时间:{}\\t 回复数量:{}\\n'.format(\n",
    "                post['title'], post['link'], post['name'], post['time'], post['replyNum']))\n",
    "            \n",
    "        print('当前网页爬取完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(url_0, deep):\n",
    "    \n",
    "    url_list = [] #将要爬取的url存入列表\n",
    "\n",
    "    for i in range(0,deep):\n",
    "        url_list.append(url_0 +'&pn=' + str(50 * i))\n",
    "    print('所有网页已经下载到本地了，开始爬虫吧')\n",
    "    \n",
    "    #循环写入信息\n",
    "    for url in url_list:\n",
    "        post = get_content(url)\n",
    "        outToFile(post)\n",
    "    print('所有信息都爬取并保存完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有网页已经下载到本地了，开始爬虫吧\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有信息都爬取并保存完毕\n"
     ]
    }
   ],
   "source": [
    "url_0 = 'https://tieba.baidu.com/f?kw=%E9%80%86%E6%B0%B4%E5%AF%92ol&ie=utf-8'\n",
    "deep = 3\n",
    "start(url_0, deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.爬取小说网站排行榜小说的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "小说网站: https://www.qu.la/\n",
    "排行榜所在网页：https://www.qu.la/paihangbang/\n",
    "爬取的信息：排行榜单，链接\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取网页soup\n",
    "\n",
    "def get_page(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 30)\n",
    "        r.raise_for_status\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return('Wrong in get_page()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取排行榜小说的名字和链接\n",
    "\n",
    "def get_content(url):\n",
    "    url_list = []\n",
    "    html = get_page(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    category_list = soup.find_all('div', class_ = 'index_toplist')\n",
    "    \n",
    "    #使用w+方式，新建读写文件，覆盖原文件\n",
    "    with open('novel_list.csv','w+') as f:\n",
    "        pass\n",
    "    \n",
    "    for cate in category_list:\n",
    "        #排行榜名\n",
    "        name = cate.find('div', class_ = 'toptab').span.text  \n",
    "        with open('novel_list.csv','a+') as f:\n",
    "            f.write('\\n小说种类：{}\\n'.format(name))\n",
    "        \n",
    "        #通过style属性定位排行榜\n",
    "        general_list = cate.find(style = 'display: block;')\n",
    "        #小说名和链接包含在li标签里\n",
    "        book_list = general_list.find_all('li')\n",
    "        \n",
    "        for book in book_list:\n",
    "            link = 'http://www.qu.la' + book.a.get('href')\n",
    "            title = book.a.get('title')\n",
    "            \n",
    "            #所有文章链接保存在列表里\n",
    "            url_list.append(link)\n",
    "            #使用a模式防止清空文件\n",
    "            with open('novel_list.csv', 'a', newline = '') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['小说名：{:<}'.format(title),\n",
    "                                '小说链接：{:<}'.format(link)])\n",
    "\n",
    "        return(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取单本小说所有章节链接，并创建小说文件\n",
    "\n",
    "def get_txt_url(url):\n",
    "    url_list = []\n",
    "    html = get_page(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    #章节\n",
    "    list_dd = soup.find_all('dd')\n",
    "    txt_name = soup.find('div', id = 'info').h1.text\n",
    "    with open('D:\\download\\小说\\{}.txt'.format(txt_name),'a+') as f:\n",
    "        f.write('小说标题：{}'.format(txt_name))\n",
    "    for url in list_dd:\n",
    "        url_list.append('http://www.qu.la'+url.a['href'])\n",
    "    del(url_list[0:12])\n",
    "    return url_list, txt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取单章内容并保存本地\n",
    "\n",
    "def get_one_txt(url, txt_name):\n",
    "    html = get_page(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    try:\n",
    "        title = soup.find('div', class_ = 'bookname').h1.text\n",
    "        txt = title +'\\n'+soup.find(\n",
    "            'div', id = 'content').text.replace('chaptererror();',' ').replace('\\xa0','').replace('\\ufeff','')\n",
    "        with open('D:\\download\\小说\\{}.txt'.format(txt_name),'a+') as f:\n",
    "            f.write(txt)\n",
    "            print('当前小说：{}的章节：{}已下载完成'.format(txt_name,title))\n",
    "    except:\n",
    "        print('something wrong in get_one_txt()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    url_0 = 'https://www.qu.la/paihangbang/'\n",
    "    book_url_list = get_content(url_0)\n",
    "    i,j=0,0\n",
    "    for book_url in book_url_list:\n",
    "        txt_url_list, txt_name = get_txt_url(book_url)\n",
    "        for txt_url in txt_url_list:\n",
    "            get_one_txt(txt_url, txt_name)\n",
    "            j = j+1\n",
    "            if(j>=15):\n",
    "                j=0\n",
    "                break\n",
    "        i = i + 1\n",
    "        if(i >=4):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **错误总结：**    \n",
    "1. 使用writerow时，每行之间多了一个空格，这是因为在windows这种使用\\r\\n的系统里，不用newline=''的话，会自动在行尾多添加个\\r，导致多出一个空行，即行尾为\\r\\r\\n     \n",
    "    * 解决办法：增加newline=''参数：with open('novel_list.csv','a'， newline = '') as f:    \n",
    "2. 使用writerow时，如果直接传入字符串对象，写入会变为一个字符一列！！ \n",
    "    * 解决办法：传入的参数应该改为字符串列表！！   \n",
    "3. get_content(url)函数的return参数没有加()括号，导致返回NULL    \n",
    "4. get_one_txt()函数出现错误：UnicodeEncodeError: 'gbk' codec can't encode character '\\xa0' in position 12: illegal multibyte sequence， 原因见：['\\xa0'编码问题](https://blog.csdn.net/github_35160620/article/details/53353672)和[Python里编码解码问题](https://blog.csdn.net/jim7424994/article/details/22675759)    \n",
    "    * 解决办法：用''来替换'\\xa0'和'\\ufeff'， 同时注意utf-8到unicode编码时有chaptererror();也需要替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.电影排行榜和图片批量下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "网站：http://dianying.2345.com/top/\n",
    "爬取的信息：电影名字，主演，简介，标题图\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取网页\n",
    "def get_page(url):\n",
    "    r = requests.get(url)\n",
    "    r.encoding = 'gbk'\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图片下载的代码\n",
    "def get_pic_from_url(url):\n",
    "    #从url以二进制的格式下载图片数据\n",
    "    pic_content = requests.get(url,stream=True).content\n",
    "    open('filename','wb').write(pic_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(url):\n",
    "    movies_list  = []\n",
    "    r = get_page(url)\n",
    "    soup = BeautifulSoup(r)\n",
    "    movies_list = soup.find('ul', class_ = 'picList clearfix').find_all('li')\n",
    "    movies = []\n",
    "    \n",
    "    #写入行名，使用w+直接清空重写\n",
    "    #常用 / 写相对路径，用 \\ 写绝对路径\n",
    "    with open('D:\\download\\movies\\movies.csv','w',newline = '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['电影名','主演','简介','海报'])\n",
    "        writer.writerow('')\n",
    "    \n",
    "    for movie in movies_list:\n",
    "        name = movie.find('span', class_ = 'sTit').text.strip()\n",
    "\n",
    "        #每位演员名字的分割？\n",
    "        actors = ''\n",
    "        #tag 的.content 属性可以将tag的子节点以列表的方式输出\n",
    "        actors0 = movie.find('p', class_ = 'pActor').contents\n",
    "        for actor in actors0:\n",
    "            actors = actors+actor.string+' '\n",
    "        actors = actors[4:].strip()\n",
    "\n",
    "        if movie.find('p',class_ = 'pTxt pIntroHide'):\n",
    "            brief = movie.find('p',class_ = 'pTxt pIntroHide').text[3:-7].strip()\n",
    "        else:\n",
    "            brief = movie.find('p',class_ = 'pTxt pIntroShow').text[3:].strip()\n",
    "        \n",
    "        \n",
    "        # 获取图片\n",
    "        \n",
    "        #这里提取src可以用.get('src')，也可以直接movie.find('img')['src']\n",
    "        img_url = 'http:'+movie.find('img').get('src')\n",
    "        with open('D:\\download\\movies\\ '+name+'.png','wb+') as f:\n",
    "            f.write(requests.get(img_url, stream = True).content)\n",
    "            \n",
    "        #将电影信息写入文件\n",
    "        with open('D:\\download\\movies\\movies.csv','a+',newline = '') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([name,actors,brief,img_url])\n",
    "                             \n",
    "        movies.append({\n",
    "            '电影名':name,\n",
    "            '主演':actors,\n",
    "            '简介':brief\n",
    "        })  \n",
    "        \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start():\n",
    "    url = 'http://dianying.2345.com/top/'\n",
    "    get_list(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **错误总结**    \n",
    "* 编码问题：使用r.apparent_encoding测试是GB2312，对某些生僻字的显示出现了乱码，使用GBK可以解决    \n",
    "* 电影简介有的自带完整版，有的需要点击展开全部之后才显示完整版，如何直接显示所有的完整版简介？    \n",
    "    * if条件判断一下    \n",
    "* 电影主演如果直接获取tag的text，名字之间没有了分割，如何将每位演员名字分隔开？\n",
    "    * 注意.contents的用法和字符串拼接    \n",
    "* with open('D:\\download\\movies\\ '+name+'.png','wb+') as f: 此处第一个路径字符串的最后一个\\之后必须跟一个字符 再连接name 才不会报错，不知是什么原因？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.爬取音悦台榜单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "网站：http://vchart.yinyuetai.com/vchart/trends?\n",
    "获取信息：音悦台的各个地区前20榜单，包含歌曲名，歌手，发布时间，评分，趋势\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return('Wrong in get_page('+url+')!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(url0):\n",
    "    areas = ['ALL', 'ML', 'HT', 'US', 'KR', 'JP']\n",
    "    areas_names = ['总榜', '内地篇', '港台篇', '欧美篇', '韩国篇', '日本篇']\n",
    "    #将地区代号和地区名打包成字典\n",
    "    area_dict = dict(zip(areas,areas_names))\n",
    "    with open('D:\\download\\music\\yinyuetai.csv','w+') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['地区', '歌曲名', '歌手', '发布时间','评分','趋势'])\n",
    "        \n",
    "    for area in areas:\n",
    "        area_name = area_dict[area]\n",
    "        with open('D:\\download\\music\\yinyuetai.csv','a+') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([area_name,])\n",
    "        \n",
    "        #得到不同地区的网页\n",
    "        url = url0 + 'area='+area\n",
    "        r = get_page(url)\n",
    "        soup = BeautifulSoup(r)\n",
    "        music_list = soup.find_all('li', class_ = 'vitem J_li_toggle_date')\n",
    "        for music in music_list:\n",
    "            music_name = music.find('div', class_ = 'info').h3.text.strip()\n",
    "            \n",
    "            #歌手名字分割\n",
    "            music_singer_list = music.find('p', class_ ='cc').find_all('a')\n",
    "            music_singer = ''\n",
    "            for singer in music_singer_list:\n",
    "                music_singer = music_singer+singer.string.strip()+' & '\n",
    "            music_singer = music_singer[:-3]\n",
    "            \n",
    "            music_time = music.find('p', class_ = 'c9').text[5:]\n",
    "            music_score = music.find('h3').text\n",
    "            music_tendency = music.find('span').text\n",
    "            \n",
    "            with open('D:\\download\\music\\yinyuetai.csv','a+',newline = '') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['', music_name, music_singer, music_time, \n",
    "                                 music_score,music_tendency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    url = 'http://vchart.yinyuetai.com/vchart/trends?'\n",
    "    get_list(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **错误总结：**\n",
    "* 歌手名字分割\n",
    "* 注意class里复制的时候，末尾的空格问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
