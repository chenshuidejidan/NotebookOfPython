{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、BeautifulSoup 爬虫\n",
    "\n",
    "   * requests 用来获取页面内容\n",
    "   * BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 安装           \n",
    "pip install requests         \n",
    "pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.爬取链家租房的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取url页面内容\n",
    "def get_page(url,headers):\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    return soup\n",
    "\n",
    "#获取整页的每个租房信息的链接\n",
    "def get_links(link_url,headers):\n",
    "    soup = get_page(link_url,headers)\n",
    "    links_a = soup.find_all('a' , attrs = {'class':'content__list--item--aside'})\n",
    "    links = [('https://cd.lianjia.com'+ link.get('href')) for link in links_a]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取房屋的数据\n",
    "def get_house_info(house_url):\n",
    "    headers['Referer']=house_url\n",
    "    soup = get_page(house_url,headers)\n",
    "    try:\n",
    "        name = soup.find('p', class_ = 'content__title').text\n",
    "        price = soup.find('p', class_ = 'content__aside--title').text.strip()\n",
    "        #text与.string的区别 https://zhuanlan.zhihu.com/p/30911642\n",
    "        #个人建议基本用.text不容易出错\n",
    "\n",
    "        #unit = soup.find('span', class_ = 'unit').text.strip()  #价格单位并去除首尾格式\n",
    "        house_tag0 = soup.find('p', class_ = 'content__aside--tags').text.strip().split('\\n')\n",
    "        house_tag = ''\n",
    "        for tag in house_tag0 :\n",
    "            house_tag = house_tag + tag + ' '\n",
    "\n",
    "        house_info = soup.find('p', class_ = 'content__article__table').text.strip().split('\\n')\n",
    "        way = house_info[0]\n",
    "        layout = house_info[1]\n",
    "        area = house_info[2]\n",
    "        direction = house_info[3]\n",
    "\n",
    "        #楼层信息在房屋信息栏里，需要切片访问\n",
    "        floor = soup.find('div',  class_  =  'content__article__info').text.strip().split('\\n')[9][3:]\n",
    "    except:\n",
    "        return {}\n",
    "    info = {\n",
    "        '标题':name,\n",
    "        '价格': price,\n",
    "        '标签':house_tag,        \n",
    "        '租法':way,\n",
    "        '户型':layout,\n",
    "        '面积':area,        \n",
    "        '朝向':direction, \n",
    "        '楼层':floor,\n",
    "    }\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的cdlianjia.txt文件中\n",
    "    '''\n",
    "    if dict:\n",
    "        with open('cdlianjia.txt','a+',encoding='utf-8') as f:\n",
    "            f.write('标题:{}\\t 价格:{}\\t 标签:{}\\t 租法:{}\\t 户型:{}\\t 面积:{}\\t 朝向:{}\\t 楼层:{}\\n'.\n",
    "                    format(dict['标题'],dict['价格'], dict['标签'], dict['租法'], dict['户型'], \n",
    "                           dict['面积'], dict['朝向'], dict['楼层']))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(deep):\n",
    "    #构造headers模拟真实浏览器访问\n",
    "    headers = {\n",
    "        'Host': \"cd.lianjia.com\",\n",
    "        'Accept': \"application/json, text/javascript, */*; q=0.01\",\n",
    "        'Accept-Encoding': \"gzip, deflate, br\",\n",
    "        'Accept-Language': \"zh-CN,zh;q=0.9\",\n",
    "        'Referer': 'https://cd.lianjia.com/zufang/',\n",
    "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\",\n",
    "        'Connection': \"keep-alive\",\n",
    "    }\n",
    "    url_list = [('https://cd.lianjia.com/zufang/'+'pg'+str(k) +'/') for k in range(1,deep+1)]\n",
    "    print('已获取链接，开始爬虫')\n",
    "    for url in url_list:\n",
    "        headers['Referer'] = url\n",
    "        links = get_links(url, headers)  #得到每个房屋的链接        \n",
    "        for link in links :\n",
    "            house_info = get_house_info(link)  #得到单个租房信息字典\n",
    "            outToFile(house_info)\n",
    "        print('当前网页爬取完毕')\n",
    "    print('所有页已爬取完毕')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已获取链接，开始爬虫\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有页已爬取完毕\n"
     ]
    }
   ],
   "source": [
    "start(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误总结：    \n",
    "1. get('href')写成了get('herf')，于是截取不到链接    \n",
    "2. 'NoneType' object has no attribute 'text'错误，是我们提取的时候有空的信息，要进行判断处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.爬取百度贴吧逆水寒吧的帖子信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取网页\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        # 这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用如下代码，但是该编码方式不一定准确：\n",
    "        #r.encoding = r.apparent_encoding\n",
    "        r.encoding = 'utf-8'\n",
    "        return r.text\n",
    "        #html=r.content\n",
    "        #html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "        #return html_doc\n",
    "    except:\n",
    "        return \" ERROR \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们知道 r.text 返回的是Unicode型的数据。    \n",
    "使用r.content返回的是bytes型的数据。   \n",
    "也就是说，如果你想取文本，可以通过r.text。  \n",
    "如果想取图片，文件，则可以通过r.content。   \n",
    "\n",
    "<pre/>\n",
    "    r = requests.get(url, timeout = 30)\n",
    "    r.raise_for_status()\n",
    "    html=r.content\n",
    "    html_doc=str(html,'utf-8')         #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    return html_doc\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取信息\n",
    "def get_content(url):\n",
    "    \n",
    "    #初始化一个列表用来保存所有帖子信息\n",
    "    posts = []\n",
    "    #下载网页到本地\n",
    "    html_doc = get_html(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    #a = requests.get(url)\n",
    "    #html=a.content\n",
    "    #html_doc=str(html,'utf-8') \n",
    "    #html_doc=html.decode(\"utf-8\",\"ignore\")\n",
    "    #做一锅汤\n",
    "    soup = BeautifulSoup(html_doc)\n",
    "\n",
    "    #找到所有具有 'j_thread_list clearfix'属性的li标签，存入列表\n",
    "    liTags = soup.find_all('li', attrs={'class':'j_thread_list'} )\n",
    "\n",
    "    #寻找信息\n",
    "\n",
    "    for li in liTags:\n",
    "        #字典存储文章信息\n",
    "        post = {}\n",
    "\n",
    "        #使用try 防止爬虫找不到信息从而停止运行\n",
    "        try :\n",
    "            '''\n",
    "            post['title'] = li.find('a', attrs = {'class':'threadlist_title pull_left j_th_tit '}).text.strip()\n",
    "            post['link'] = \"http://tieba.baidu.com/\"+li.find('a', attrs={'class': 'threadlist_title pull_left j_th_tit '})['href']\n",
    "            post['name'] = li.find('span', attrs={'class': 'tb_icon_author '}).text.strip()\n",
    "            post['time'] = li.find('span', attrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            post['replyNum'] = li.find('span', sttrs = {'class':'threadlist_reply_date pull_right j_reply_data'}).text.strip()\n",
    "            '''\n",
    "            post['title'] = li.find('a', class_='j_th_tit').text\n",
    "            post['link'] = 'http://tieba.baidu.com' + li.find('a', class_='j_th_tit').get('href')\n",
    "            post['name'] = li.find('a', class_= 'j_user_card').text\n",
    "            post['time'] = li.find('span', class_='is_show_create_time').text\n",
    "            post['replyNum'] = li.find('span', class_='threadlist_rep_num').text\n",
    "            posts.append(post)\n",
    "        except:\n",
    "            print('出问题了~')\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取的文件写入本地\n",
    "def outToFile(dict):\n",
    "    '''\n",
    "    写入本地当前目录的nshba.txt文件中\n",
    "    '''\n",
    "    with open('nshba.txt','a+',encoding='utf-8') as f:\n",
    "        for post in dict:\n",
    "            f.write('标题:{}\\t 链接:{}\\t 发帖人:{}\\t 最后回复时间:{}\\t 回复数量:{}\\n'.format(\n",
    "                post['title'], post['link'], post['name'], post['time'], post['replyNum']))\n",
    "            \n",
    "        print('当前网页爬取完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(url_0, deep):\n",
    "    \n",
    "    url_list = [] #将要爬取的url存入列表\n",
    "\n",
    "    for i in range(0,deep):\n",
    "        url_list.append(url_0 +'&pn=' + str(50 * i))\n",
    "    print('所有网页已经下载到本地了，开始爬虫吧')\n",
    "    \n",
    "    #循环写入信息\n",
    "    for url in url_list:\n",
    "        post = get_content(url)\n",
    "        outToFile(post)\n",
    "    print('所有信息都爬取并保存完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有网页已经下载到本地了，开始爬虫吧\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "当前网页爬取完毕\n",
      "所有信息都爬取并保存完毕\n"
     ]
    }
   ],
   "source": [
    "url_0 = 'https://tieba.baidu.com/f?kw=%E9%80%86%E6%B0%B4%E5%AF%92ol&ie=utf-8'\n",
    "deep = 3\n",
    "start(url_0, deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
